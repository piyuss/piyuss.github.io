<!DOCTYPE html>
<html lang="en">
    
    <head>
        <title>Piyush Pandey</title>
        <meta charset="utf-8">
        <meta name="description" content="This is Piyush pandey's academic portfolio. Fields of interest: Engineering, Agricultural robotics, Computer Vision, NCSU, PHD">
        <meta name="author" content="Anugya Sharma">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="refresh" content="600">
        <meta name="keywords" content="Piyush Pandey">        
        <link rel="stylesheet" type="text/css" href="./stylesheets/style.css">  
        <link rel="icon" type="img/x-icon" href="/img/fir.png">        
    </head>

    <body>
        <img class= "cursive" src="img/cursive-text.png">     

        <central>
        </central>
        

        
        <header>             
            <nav>
                <ul class="navigation">
                    <li><a href="index.html" style= "background-color:#86c7af; border-radius:5px; color:#f2ebe8; ">HOME</a></li>                    
                    <!-- <li><a href="resume.html">CV</a></li> -->
                    <li><a href="publications.html">PUBLICATIONS</a></li>
                    <li><a href="presentations.html">PRESENTATIONS</a></li>                    
                </ul>
            </nav>
        </header>

        <!-- main content /white paper -->
        <main>
            <h1>About me</h1>
            <p>Hello. I am an ORISE postdoctoral fellow at USDA-ARS working on plant phenotyping using deep learning methods.
                
                I completed my dual PhD in Engineering and Forestry at North Carolina State University. 
            </p>

            <p> I have worked on projects in high-throughput phenotyping of plants using digital images and 
                in robotic systems for forestry applications. Some of the projects are visualized
                in the slideshow below. Please use the links to publications and presentations on the navigation bar for
                 a more complete view of my work.
            </p>
            <p> I have worked on projects in high-throughput phenotyping of plants using digital images and 
                in robotic systems for forestry applications. Some of the projects are visualized
                in the slideshow below. Please use the links to publications and presentations on the navigation bar for
                 a more complete view of my work.
            </p>
        </main>
        

        <photoArea>
            <img src="img/bio-photo.png">
            <h1>Piyush Pandey</h1>

            <mediaLinks>                
                <a href="mailto:piyush.pandey@usda.gov" target="_blank"><img src="img/email-icon.png"></a>
                <a href="https://www.linkedin.com/in/piyush-pandey11" target="_blank"><img src="img/linkedin-icon.png"></a>
                <!-- https://www.linkedin.com/in/piyush-pandey11 -->
                <a href="https://github.com/piyuss" target="_blank"><img src="img/github-icon.png"></a>
                <a href="https://twitter.com/PiyushPandey__" target="_blank"><img src="img/twitter-icon.png"></a>
                <a href="https://scholar.google.fr/citations?user=O1qUwKcAAAAJ&hl" target="_blank"><img src="img/googlescholar-icon.png"></a>
            </mediaLinks>
        </photoArea>

        
       
        
        <projectArea>
            <!-- 1 -->
            <ProjectRowContainer>
                <h1>Synthetic Images for Model Training</h1>
                <projectRow>
                    <projectRowItem1>
                        <img class = "project_img" width="354"  src="project_descriptions/14_synthetic_images.png"> 
                    </projectRowItem1>
                    <projectRowItem2>
                        <p>
                            The detection of individual plants within field images is critical for many applications in precision agriculture and research. Computer vision models for object detection, while often highly accurate, require large amounts of labeled data for training, something that is not readily available for most plants. To address the challenge of creating large datasets with accurate labels, we used indoor images of maize plants to create synthetic field images with automatically derived bounding box labels, enabling the generation of thousands of synthetic images without any manual labeling. Training an object detection model (Faster R-CNN) exclusively on synthetic images led to a mean average precision (mAP) value of 0.533 when the model was evaluated on pre-processed real plot images. When fine-tuned with a small number of real plot images, the model pre-trained on the synthetic images (mAP=0.884) outperformed the model that was not pre-trained.
                        </p>
                    </projectRowItem2>
                </projectRow>
                <hr>
            </ProjectRowContainer>

            <!-- 2 -->
            <ProjectRowContainer>
                <h1>Robotic manipulation of sweet potato slips</h1>
                <projectRow>
                    <projectRowItem1>
                        <img width="354"  src="project_descriptions/13_sweet_potato_slips.png"> 
                    </projectRowItem1>
                    <projectRowItem2>
                        <p>
                            This mini-project was focused on prototyping a sweet potato slip handling system. This was aimed at automating slip handling during sweet potato planting. I worked on the creation of an imaging system to develop a computer vision system that can accurately determine the position of individual slip. I used a Mynt Eye stereo camera was used, and created a simple GUI using R-QT to trigger the camera and record images. The mechanical system needed a manipulator to pick up the delicate slips. I designed some end effector geometries in Solidworks and 3D printed them for tests. I also later supervised an undergraduate student to further develop this project for picking up the slips.
                        </p>
                    </projectRowItem2>
                </projectRow>
                <hr>
            </ProjectRowContainer>

            <!-- 3 -->
            <ProjectRowContainer>
                <h1>(Tabletop) Raspberry Cane Pruning Robot</h1>
                <projectRow>
                    <projectRowItem1>
                        <img  class = "project_img" width="354"  src="project_descriptions/3_asabe_robot.png"> 
                    </projectRowItem1>
                    <projectRowItem2>
                        <p>
                            This was a student robotics competition where I was a team member at the University of Nebraska - Lincoln. The robot was supposed to walk down the table and for every "plot" of "raspberry canes" on the table, it needed to trim only a certain number of green or yellow canes. We had a robot arm with blades on its end effector for cutting the canes and an articulated crank mechanism that would send a Raspberry Pi camera to the top of the plot for image acquisition. 
                            
                            </p>
                            <p>
                            
                            I was in charge of the vision system and my specific responsibility was to use the images and produce 3D coordinates for each cane so that the arm could go and cut them. I was initially planning to use a depth camera but then realized that the grid of canes resembles a camera calibration checkerboard. The size of the grid and the distance between the canes was fixed, so once I detected the canes and the empty holes, all I needed was the camera intrinsics to find the 3D coordinates. We got the second prize in the competition.
                        </p>
                        <p>
                            The UNL news article about this competition can be found <a href="https://web.archive.org/web/20220627175938/https://newsroom.unl.edu/announce/bseunl/6909/38774" target="_blank">here</a>. 
                        </p>
                    </projectRowItem2>
                </projectRow>
                <hr>
            </ProjectRowContainer>

            <!-- 3 -->
            <ProjectRowContainer>
                <h1>Predicting lettuce nutrients using hyperspectral imaging</h1>
                <projectRow>
                    <projectRowItem1>
                        <img class = "project_img" width="354"  src="project_descriptions/10_hydroponic_lettuce.png"> 
                    </projectRowItem1>
                    <projectRowItem2>
                        <p>
                             This study investigated \textit{in situ} hyperspectral imaging of hydroponic lettuce for predicting nutrient concentrations and identifying nutrient deficiencies for: nitrogen (N), phosphorous (P), potassium (K), calcium (Ca), magnesium (Mg), and {\color{red}{sulphur}} (S). Plants were imaged using a hyperspectral line scanner at six and eight weeks after transplanting, then plant tissue was sampled, and nutrient concentrations measured. Partial least squares regression (PLSR) models were developed to predict nutrient concentrations for each nutrient individually (PLS1) and for all six nutrient concentrations (PLS2). Several binary classification models were also developed to predict nutrient deficiencies. The PLS1 and PLS2 models predicted nutrient concentrations with R^2 values from 0.60-0.88 for N, P, K, and S, while results for Ca and Mg yielded R^2 values of 0.12-0.34, for both harvest dates. Similarly, plants deficient in N, P, K, and S were classified more accurately compared to plants deficient in Ca and Mg for both harvest dates, with F1 values (F-scores) ranging from 0.71 to 1.00, with the exception of K which had F1 scores of 0.40-0.67. Overall, results indicate that both leaf tissue nutrient concentration and nutrient deficiencies can be predicted using hyperspectral data collected in-vivo. 
                        </p>
                    </projectRowItem2>
                </projectRow>
                <hr>
            </ProjectRowContainer>

                       <!-- 3 -->
                       <ProjectRowContainer>
                        <h1>Manually operated citrus grading machine</h1>
                        <projectRow>
                            <projectRowItem1>
                                <img class = "project_img" width="354"  src="project_descriptions/1_orange_grader.png"> 
                            </projectRowItem1>
                            <projectRowItem2>
                                <p>
                                    The citrus grading machine was a part of my undergraduate research thesis on machine design. The problem was to design a portable fruit grading machine that could work without electric power. The solution uses a series of inclined tubes with the spacing between them gradually increasing as we go down the incline. The smallest fruits will slip down the gaps first and the largest fruits roll down all the way to the foot of the incline. The inclined tubes have a manually operated rotating mechanism that perturbs the fruits. This is to avoid the fruits from getting stuck due to friction. The angle of inclination had to be based on iterative experimentation to find out the coefficient of friction between the fruits and the inclined tubes. The system also needed careful design to ensure gentle landing of the fruits after slipping through the gaps.
                                </p>
                            </projectRowItem2>
                        </projectRow>
                        <hr>
                    </ProjectRowContainer>
        </projectArea>


       
        <footer> <p>© 2023 Piyush Pandey</p> </footer>
        <!-- <script src="script/script1_carousel.js"></script> -->
    </body>

</html>